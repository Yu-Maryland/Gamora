{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from subprocess import check_output\n",
    "\n",
    "root_folder = os.path.abspath(os.path.dirname(os.path.dirname(os.path.abspath(\"build_ds.ipynb\"))))\n",
    "sys.path.append(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import json\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from utils.torch_util import all_numpy\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetSaver for node-level classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSaver(object):\n",
    "    def __init__(self, dataset_name, root = '', version = 1):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.root = root\n",
    "        \n",
    "        self.dataset_dir = osp.join(self.root, self.dataset_name) \n",
    "        \n",
    "        if osp.exists(self.dataset_dir):\n",
    "            if input(f'Found an existing directory at {self.dataset_dir}/. \\nWill you remove it? (y/N)\\n').lower() == 'y':\n",
    "                shutil.rmtree(self.dataset_dir)\n",
    "                print('Removed existing directory')\n",
    "            else:\n",
    "                print('Process stopped.')\n",
    "                exit(-1)\n",
    "                \n",
    "        # make necessary dirs\n",
    "        self.raw_dir = osp.join(self.dataset_dir, 'raw')\n",
    "        os.makedirs(self.raw_dir, exist_ok=True)\n",
    "        os.makedirs(osp.join(self.dataset_dir, 'processed'), exist_ok=True)\n",
    "\n",
    "        # create release note\n",
    "        with open(osp.join(self.dataset_dir, f'RELEASE_v{version}.txt'), 'w') as fw:\n",
    "            fw.write(f'# Release note for {self.dataset_name}\\n\\n### v{version}: {date.today()}')\n",
    "        \n",
    "        # check list\n",
    "        self._save_graph_list_done = False\n",
    "        self._save_split_done = False\n",
    "    \n",
    "    def save_graph_list(self, graph_list):\n",
    "        dict_keys = graph_list[0].keys()\n",
    "        if not 'edge_index' in dict_keys:\n",
    "            raise RuntimeError('edge_index needs to be provided in graph objects')\n",
    "        if not 'num_nodes' in dict_keys:\n",
    "            raise RuntimeError('num_nodes needs to be provided in graph objects')\n",
    "        \n",
    "        print(dict_keys)\n",
    "        \n",
    "        data_dict = {}\n",
    "        # Store the following keys\n",
    "        # - edge_index (necessary)\n",
    "        # - num_nodes_list (necessary)\n",
    "        # - num_edges_list (necessary)\n",
    "        # - node_** (optional, node_feat is the default node features)\n",
    "        # - edge_** (optional, edge_feat is the default edge features)\n",
    "        \n",
    "        # saving num_nodes_list\n",
    "        print('Saving num-node-list.csv.gz')\n",
    "        num_nodes_list = np.array([graph['num_nodes'] for graph in graph_list]).astype(np.int64)\n",
    "        data_dict['num_nodes_list'] = num_nodes_list\n",
    "        \n",
    "        print('Saving edge.csv.gz and num-edge-list.csv.gz')\n",
    "        edge_index = np.concatenate([graph['edge_index'] for graph in graph_list], axis = 1).astype(np.int64)\n",
    "        num_edges_list = np.array([graph['edge_index'].shape[1] for graph in graph_list]).astype(np.int64)\n",
    "        if edge_index.shape[0] != 2:\n",
    "            raise RuntimeError('edge_index must have shape (2, num_edges)')\n",
    "        data_dict['edge_index'] = edge_index\n",
    "        data_dict['num_edges_list'] = num_edges_list\n",
    "        \n",
    "        for key in dict_keys:\n",
    "            if key == 'edge_index' or key == 'num_nodes':\n",
    "                continue \n",
    "            if graph_list[0][key] is None:\n",
    "                continue\n",
    "\n",
    "            if 'node_' == key[:5]:\n",
    "                # make sure saved in np.int64 or np.float32\n",
    "                dtype = np.int64 if 'int' in str(graph_list[0][key].dtype) else np.float32\n",
    "                # check num_nodes\n",
    "                for i in range(len(graph_list)):\n",
    "                    if len(graph_list[i][key]) != num_nodes_list[i]:\n",
    "                        raise RuntimeError(f'num_nodes mistmatches with {key}')\n",
    "\n",
    "                cat_feat = np.concatenate([graph[key] for graph in graph_list], axis = 0).astype(dtype)\n",
    "                data_dict[key] = cat_feat\n",
    "\n",
    "            elif 'edge_' == key[:5]:\n",
    "                # make sure saved in np.int64 or np.float32\n",
    "                dtype = np.int64 if 'int' in str(graph_list[0][key].dtype) else np.float32\n",
    "                # check num_edges\n",
    "                for i in range(len(graph_list)):\n",
    "                    if len(graph_list[i][key]) != num_edges_list[i]:\n",
    "                        raise RuntimeError(f'num_edges mistmatches with {key}')\n",
    "\n",
    "                cat_feat = np.concatenate([graph[key] for graph in graph_list], axis = 0).astype(dtype)\n",
    "                data_dict[key] = cat_feat\n",
    "\n",
    "            else:\n",
    "                raise RuntimeError(f'Keys in graph object should start from either \\'node_\\' or \\'edge_\\', but \\'{key}\\' given.')\n",
    "\n",
    "        self.has_node_attr = ('node_feat' in graph_list[0]) and (graph_list[0]['node_feat'] is not None)\n",
    "        self.has_edge_attr = ('edge_feat' in graph_list[0]) and (graph_list[0]['edge_feat'] is not None)\n",
    "        \n",
    "        # num-node-list, num-edge-list\n",
    "        n_node_list = pd.DataFrame(data_dict['num_nodes_list'])\n",
    "        n_edge_list = pd.DataFrame(data_dict['num_edges_list'])\n",
    "\n",
    "        n_node_list.to_csv(self.raw_dir + '/num-node-list.csv', index = False, header = False)\n",
    "        n_node_list.to_csv(self.raw_dir + '/num-node-list.csv.gz', index = False, header = False, compression='gzip')\n",
    "        n_edge_list.to_csv(self.raw_dir + '/num-edge-list.csv', index = False, header = False)\n",
    "        n_edge_list.to_csv(self.raw_dir + '/num-edge-list.csv.gz', index = False, header = False, compression='gzip')\n",
    "        \n",
    "        # edge list\n",
    "        EDGE_list = pd.DataFrame(data_dict['edge_index'].transpose())\n",
    "        EDGE_list.to_csv(self.raw_dir + '/edge.csv', index = False, header = False)\n",
    "        EDGE_list.to_csv(self.raw_dir + '/edge.csv.gz', index = False, header = False, compression='gzip')\n",
    "\n",
    "        # node-feat\n",
    "        if self.has_node_attr:\n",
    "            print('Saving node-feat.csv.gz')\n",
    "            NODE = pd.DataFrame(data_dict['node_feat'])\n",
    "            NODE.to_csv(self.raw_dir + '/node-feat.csv', index = False, header = False)\n",
    "            NODE.to_csv(self.raw_dir + '/node-feat.csv.gz', index = False, header = False, compression='gzip')\n",
    "        \n",
    "        if self.has_edge_attr:\n",
    "            print('Saving edge-feat.csv.gz')\n",
    "            EDGE_feat = pd.DataFrame(data_dict['edge_feat'])\n",
    "            EDGE_feat.to_csv(self.raw_dir + '/edge-feat.csv', index = False, header = False)\n",
    "            EDGE_feat.to_csv(self.raw_dir + '/edge-feat.csv.gz', index = False, header = False, compression='gzip')\n",
    "\n",
    "        print('Saved all the files!')\n",
    "        self._save_graph_list_done = True\n",
    "        self.num_data = graph_list[0]['num_nodes']\n",
    "\n",
    "    def save_target_labels(self, target_labels):\n",
    "        '''\n",
    "            target_label (numpy.narray): storing target labels. Shape must be (num_data, num_tasks)\n",
    "        '''\n",
    "    \n",
    "        if not self._save_graph_list_done:\n",
    "            raise RuntimeError('save_graph_list must be done beforehand.')\n",
    "\n",
    "        # check type and shape\n",
    "        if not isinstance(target_labels, np.ndarray):\n",
    "            raise ValueError(f'target label must be of type np.ndarray')\n",
    "\n",
    "        if len(target_labels) != self.num_data:\n",
    "            raise RuntimeError(f'The length of target_labels ({len(target_labels)}) must be the same as the number of data points ({self.num_data}).')\n",
    "\n",
    "        node_label = pd.DataFrame(target_labels)\n",
    "        node_label.to_csv(self.raw_dir + '/node-label.csv.gz', index = False, header = False, compression='gzip')\n",
    "        node_label.to_csv(self.raw_dir + '/node-label.csv', index = False, header = False)\n",
    "        \n",
    "        self.num_tasks = target_labels.shape[1]\n",
    "\n",
    "        self._save_target_labels_done = True\n",
    "    \n",
    "    \n",
    "    def save_split(self, split_dict, split_name = 'random'):\n",
    "        '''\n",
    "            Save dataset split\n",
    "                split_dict: must contain three keys: 'train', 'valid', 'test', where the values are the split indices stored in numpy.\n",
    "                split_name (str): the name of the split\n",
    "        '''\n",
    "\n",
    "        self.split_dir = osp.join(self.dataset_dir, 'split', split_name)\n",
    "        os.makedirs(self.split_dir, exist_ok = True)\n",
    "        \n",
    "        # verify input\n",
    "        if not 'train' in split_dict:\n",
    "            raise ValueError('\\'train\\' needs to be given in save_split')\n",
    "        if not 'valid' in split_dict:\n",
    "            raise ValueError('\\'valid\\' needs to be given in save_split')\n",
    "        if not 'test' in split_dict:\n",
    "            raise ValueError('\\'test\\' needs to be given in save_split')\n",
    "\n",
    "        if not all_numpy(split_dict):\n",
    "            raise RuntimeError('split_dict must only contain list/dict of numpy arrays, int, or float')\n",
    "        \n",
    "        test_list = pd.DataFrame(split_dict['test'])\n",
    "        train_list = pd.DataFrame(split_dict['train'])\n",
    "        valid_list = pd.DataFrame(split_dict['valid'])\n",
    "\n",
    "        test_list.to_csv(self.split_dir + '/test.csv', index = False, header = False)\n",
    "        train_list.to_csv(self.split_dir + '/train.csv', index = False, header = False)\n",
    "        valid_list.to_csv(self.split_dir + '/valid.csv', index = False, header = False)\n",
    "\n",
    "        test_list.to_csv(self.split_dir + '/test.csv.gz', index = False, header = False, compression='gzip')\n",
    "        train_list.to_csv(self.split_dir + '/train.csv.gz', index = False, header = False, compression='gzip')\n",
    "        valid_list.to_csv(self.split_dir + '/valid.csv.gz', index = False, header = False, compression='gzip')\n",
    "\n",
    "        self.split_name = split_name\n",
    "        self._save_split_done = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"mult8_raw/mult8-class_map.json\"\n",
    "file_edge_list = \"mult8_raw/mult8.el\"\n",
    "file_node_feat = \"mult8_raw/mult8-feats.csv\"\n",
    "save_dir = \"mult8/\"\n",
    "graph_list = []\n",
    "\n",
    "\n",
    "# build graphs\n",
    "fh = open(file_edge_list, \"rb\")\n",
    "g = nx.read_edgelist(fh, create_using = nx.DiGraph, nodetype=int)\n",
    "fh.close()\n",
    "graph = dict()\n",
    "graph['edge_index'] = np.array(g.edges).transpose() \n",
    "graph['num_nodes'] = len(g.nodes)\n",
    "feats = np.loadtxt(file_node_feat, delimiter=',')\n",
    "graph['node_feat'] = np.array(feats)\n",
    "\n",
    "graph_list.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset directory\n",
    "saver = DatasetSaver('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['edge_index', 'num_nodes', 'node_feat'])\n",
      "Saving num-node-list.csv.gz\n",
      "Saving edge.csv.gz and num-edge-list.csv.gz\n",
      "Saving node-feat.csv.gz\n",
      "Saved all the files!\n"
     ]
    }
   ],
   "source": [
    "# save graph info\n",
    "saver.save_graph_list(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### node label\n",
    "# read node label\n",
    "f_class = open(class_name)\n",
    "v = json.load(f_class).values()\n",
    "f_class.close()\n",
    "size = len(list(v))\n",
    "labels = np.argmax(np.array(list(v)), axis = 1).reshape(size, 1)\n",
    "\n",
    "# save node labels\n",
    "saver.save_target_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train, valid, and test\n",
    "split_idx = dict()\n",
    "perm = np.random.permutation(saver.num_data)\n",
    "split_idx['train'] = perm[: int(0.8 * saver.num_data)]\n",
    "split_idx['valid'] = perm[int(0.8 * saver.num_data): int(0.9 * saver.num_data)]\n",
    "split_idx['test'] = perm[int(0.9 * saver.num_data):]\n",
    "saver.save_split(split_idx, split_name = 'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually building dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"mult8_raw/mult8-class_map.json\"\n",
    "file_edge_list = \"mult8_raw/mult8.el\"\n",
    "file_node_feat = \"mult8_raw/mult8-feats.csv\"\n",
    "save_dir = \"mult8/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read node labels\n",
    "f_class = open(class_name)\n",
    "v = json.load(f_class).values()\n",
    "f_class.close()\n",
    "size = len(list(v))\n",
    "labels = np.argmax(np.array(list(v)), axis = 1).reshape(size, 1)\n",
    "\n",
    "# save node labels\n",
    "node_label = pd.DataFrame(labels.flatten())\n",
    "node_label.to_csv(save_dir + 'node-label.csv.gz', index = False, header = False, compression='gzip')\n",
    "node_label.to_csv(save_dir + 'node-label.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build graphs\n",
    "fh = open(file_edge_list, \"rb\")\n",
    "g = nx.read_edgelist(fh, create_using = nx.DiGraph, nodetype=int)\n",
    "fh.close()\n",
    "\n",
    "feats = np.loadtxt(file_node_feat, delimiter=',')\n",
    "d = dict(enumerate(feats.tolist(), 0))\n",
    "nx.set_node_attributes(g, d, \"feat\")\n",
    "\n",
    "\n",
    "num_node = [len(g.nodes)]\n",
    "num_edge = [len(g.edges)]\n",
    "\n",
    "node_feat=[]\n",
    "edge_list=[]\n",
    "edge_feat=[]\n",
    "\n",
    "for i in range(len(g.nodes)):\n",
    "    node_feat.append(list(g.nodes[i]['feat']))\n",
    "    \n",
    "for e in g.edges:\n",
    "    source = e[0]\n",
    "    target = e[1]\n",
    "    edge_list.append([source, target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 440, 440, 440],\n",
       "       [ 16,  18,  30, ..., 453, 454, 455]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(g.edges).transpose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []\n",
    "num_data = 100\n",
    "for i in range(num_data):\n",
    "    g = nx.fast_gnp_random_graph(10, 0.5)\n",
    "    graph = dict()\n",
    "    graph['edge_index'] = np.array(g.edges).transpose() \n",
    "    num_edges = graph['edge_index'].shape[1]\n",
    "\n",
    "    graph['num_nodes'] = len(g.nodes)\n",
    "    # optionally, add node/edge features\n",
    "    graph['node_feat'] = np.random.randn(graph['num_nodes'], 3)\n",
    "    graph['edge_feat'] = np.random.randn(num_edges, 3) \n",
    "    \n",
    "    graph_list.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['edge_index', 'num_nodes', 'node_feat', 'edge_feat'])\n"
     ]
    }
   ],
   "source": [
    "print(graph_list[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save node feats and edge lists\n",
    "NODE = pd.DataFrame(node_feat)\n",
    "EDGE_list = pd.DataFrame(edge_list)\n",
    "#EDGE_feat=pd.DataFrame(edge_feat)\n",
    "\n",
    "NODE.to_csv(save_dir + 'node-feat.csv', index = False, header = False)\n",
    "NODE.to_csv(save_dir + 'node-feat.csv.gz', index = False, header = False, compression='gzip')\n",
    "EDGE_list.to_csv(save_dir + 'edge.csv', index = False, header = False)\n",
    "EDGE_list.to_csv(save_dir + 'edge.csv.gz', index = False, header = False, compression='gzip')\n",
    "#EDGE_feat.to_csv('edge-feat.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save num of nodes and num od edges\n",
    "num_node_list = pd.DataFrame(num_node)\n",
    "num_edge_list = pd.DataFrame(num_edge)\n",
    "\n",
    "num_node_list.to_csv(save_dir + 'num-node-list.csv', index = False, header = False)\n",
    "num_node_list.to_csv(save_dir + 'num-node-list.csv.gz', index = False, header = False, compression='gzip')\n",
    "num_edge_list.to_csv(save_dir + 'num-edge-list.csv', index = False, header = False)\n",
    "num_edge_list.to_csv(save_dir + 'num-edge-list.csv.gz', index = False, header = False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "num = len(g.nodes)\n",
    "train_valid, test= model_selection.train_test_split([i for i in range(num)], train_size=0.8, test_size = None)\n",
    "train, valid = model_selection.train_test_split(train_valid, train_size=0.9, test_size = None)\n",
    "test_list = pd.DataFrame(sorted(test))\n",
    "train_list = pd.DataFrame(sorted(train))\n",
    "valid_list = pd.DataFrame(sorted(valid))\n",
    "test_list.to_csv(save_dir + 'test.csv', index = False, header = False)\n",
    "train_list.to_csv(save_dir + 'train.csv', index = False, header = False)\n",
    "valid_list.to_csv(save_dir + 'valid.csv', index = False, header = False)\n",
    "\n",
    "test_list.to_csv(save_dir + 'test.csv.gz', index = False, header = False, compression='gzip')\n",
    "train_list.to_csv(save_dir + 'train.csv.gz', index = False, header = False, compression='gzip')\n",
    "valid_list.to_csv(save_dir + 'valid.csv.gz', index = False, header = False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric as pyg\n",
    "print(pyg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.3\n"
     ]
    }
   ],
   "source": [
    "print(nx.__version__)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f01bbd461b44d47e6849f1e1d168da3086f09e053a6a22be92cde35c4dee083"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
